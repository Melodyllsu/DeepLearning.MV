
1.	引言
Q1 ：什么是卷积神经网络？及其作用。
卷积神经网络（Convolutional Neural Network, CNNs / ConvNets）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。
 
注：该图展示卷积神经网络可以识别场景，也可以提供相关的标签，比如“桥梁”、“火车”和“网球”。

 
注：该图展示了卷积神经网络可以用来检测日常物体、人和动物。

注：卷积神经网络也在一些自然语言处理任务（比如语句分类）。


Q2：为什么引入卷积神经网络？
在人工的全连接神经网络中，每相邻两层之间的每个神经元之间都是有边相连的。当输入层的特征维度变得很高时，这时全连接网络需要训练的参数就会增大很多，计算速度就会变得很慢，例如一张黑白的 28×28 的手写数字图片，输入层的神经元就有784个。效果如下图：
 
若在中间只使用一层隐藏层，参数 w 就有 784×15=11760 多个；若输入的是28×28 带有颜色的RGB格式的手写数字图片，输入神经元就有28×28×3=2352 个。这很容易看出使用全连接神经网络处理图像中的需要训练参数过多的问题。
而在卷积神经网络中，卷积层的神经元只与前一层的部分神经元节点相连，即它的神经元间的连接是非全连接的，且同一层中某些神经元之间的连接的权重 w 和偏移 b 是共享的（即相同的），这样大量地减少了需要训练参数的数量。
Q3：卷积神经网络的基本结构。
a）CNNs网络与传统网络示意图之间的比较
 	 
传统神经网络	卷积神经网络
一个卷积神经网络由很多层组成，它们的输入是三维的，输出也是三维的，有的层有参数，有的层不需要参数。
卷积神经网络具有三维体积的神经元。
卷积神经网络利用输入是图片的特点，把神经元设计成三个维度：width, height, depth(注意这个depth不是神经网络的深度，而是用来描述神经元的) 。比如输入的图片大小是 32 × 32 × 3 (rgb)，那么输入神经元就也具有 32×32×3 的维度。
b）组成CNNs的一般层次（将在第2章中详解每一层功能）
	卷积层（Convolutional layer），卷积神经网路中每层卷积层由若干卷积单元组成，每个卷积单元的参数都是通过反向传播算法优化得到的。卷积运算的目的是提取输入的不同特征，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。
	线性整流层（Rectified Linear Units layer, ReLU layer），这一层神经的活性化函数（Activation function）使用线性整流（Rectified Linear Units, ReLU）f(x)=max(0,x)。
	池化层（Pooling layer），通常在卷积层之后会得到维度很大的特征，将特征切成几个区域，取其最大值或平均值，得到新的、维度较小的特征。
	全连接层（Fully-Connected layer），把所有局部特征结合变成全局特征，用来计算最后每一类的得分。
c）CNNs工作框架
 
由上图可以看到，CNNs的输入层为原始图片，在计算机中图片就是用构成像素点的多维矩阵来表示。然后中间层包括若干层的卷积+ReLU+池化，和若干层的全连接层，这一部分是CNNs的核心，是用来对特征进行学习和组合的，最终会学到一些强特征，具体是如何学习到的会在下面给出。最后会利用中间层学到的强特征做为输入通过softmax函数来得到输出标记。

2．组成CNNs框架的各层级详解
2.1 输入层
在CNNs的输入层中，（图片）数据输入的格式与全连接神经网络的输入格式（一维向量）不太一样。CNNs的输入层的输入格式保留了图片本身的结构。
 
注：对于黑白的 28×28 的图片，CNNs的输入是一个 28×28 的的二维神经元	 
注：而对于RGB格式的28×28图片，CNNs的输入则是一个 3×28×28 的三维神经元（RGB中的每一个颜色通道都有一个 28×28 的矩阵）
2.2 卷积层
卷积层中包含两个基本的概念，来降低模型的参数数量，使得模型易于训练。这两个概念为：
	local receptive fields（局部感知野）
	shared weights（共享权值）
局部感知野
一般认为人对外界的认知是从局部到全局的，而图像的空间联系也是局部的像素联系较为紧密，而距离较远的像素相关性则较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。
假设输入的是一个 28×28 的二维神经元，我们定义5×5 的一个局部感知野，即隐藏层的神经元与输入层的5×5个神经元相连，这个5*5的区域就称之为Local Receptive Fields。
 	 
一个感受视野带有一个卷积核，我们将感受视野中的权重w矩阵称为卷积核；将感受视野对输入的扫描间隔称为步长（stride）；当步长比较大时（stride>1），为了扫描到边缘的一些特征，感受视野可能会“出界”，这时需要对边界扩充(pad)，边界扩充可以设为0或其他值。步长和边界扩充值的大小由用户来定义。
卷积核的大小由用户来定义，即定义的感受视野的大小；卷积核的权重矩阵的值，便是卷积神经网络的参数，为了有一个偏移项，卷积核可附带一个偏移项 b，它们的初值可以随机来生成，可通过训练进行变化。
因此感受视野扫描时可以计算出下一层神经元的值为：
 
对下一层的所有神经元来说，它们从不同的位置去探测了上一层神经元的特征。

权值共享
我们将通过一个带有卷积核的感受视野扫描生成的下一层神经元矩阵称为一个feature map (特征映射图)，如下左图。因此在同一个feature map上的神经元使用的卷积核是相同的，因此这些神经元shared weights，共享卷积核中的权值和附带的偏移。一个feature map对应一个卷积核，若我们使用3个不同的卷积核，可以输出3个feature map：（感受视野：5×5，步长stride：1）。如下右图。因此在CNNs的卷积层，我们需要训练的参数大大地减少到了 (5×5+1)×3=78个。
 	 

假设输入的是28×28的RGB图片，即输入的是一个3×28×28的二维神经元，这时卷积核的大小不只用长和宽来表示，还有深度，感受视野也对应的有了深度，如下图所示：
 
由图可知：
感受视野：3×2×2；卷积核：3×2×2，深度：3；
下一层的神经元的值为： . 
卷积核的深度和感受视野的深度相同，都由输入数据来决定，长宽可由自己来设定，数目也可以由自己来设定，一个卷积核依然对应一个feature map。

2.3 线性整流层
 
ReLU 是一个元素级别的操作（应用到各个像素），并将特征图中的所有小于 0 的像素值设置为零。
ReLU 操作直观效果：
 
其他非线性函数，比如tanh或者sigmoid也可以用来替代ReLU, 但ReLU在大部分情况下表现是更好的。
Q1: 为什么引入非线性激励函数？
如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了。正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。最早的想法是sigmoid函数或者tanh函数，输出有界，很容易充当下一层输入。
Q2：为什么引入ReLU？ 
第一，采用sigmoid等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大，而采用ReLU激活函数，整个过程的计算量节省很多。 
第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失），从而无法完成深层网络的训练。 
第三，ReLU会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。

2.4 池化层
池化pooling，也称为欠采样（subsampling）或下采样（downsampling），主要用于降低特征的维度，同时提高模型容错性，主要有max，average和sum等不同类型的操作。如下图对特征图进行最大池化的操作：
 
通过池化操作，使原本4×4的特征图变成了2×2，从而降低了特征维度，提高了容错性。下图给出了模型经过池化的可视化表示： 
 
池化函数可以逐渐降低输入表示的空间尺度。特点：
	使输入表示（特征维度）变得更小，并且网络中的参数和计算的数量更加可控，因此，可以控制过拟合。
	使网络对于输入图像中很小的变化、冗余和变换变得不敏感（输入的微小冗余将不会改变池化的输出）——因为我们在局部邻域中使用了最大化/平均值的操作。这就使网络的鲁棒性增强了，有一定抗扰动的作用
	帮助我们获取图像最大程度上的尺度不变性（准确的词是“不变性”）。它非常的强大，因为我们可以检测图像中的物体，无论它们位置在哪里。
2.5 全连接层
全连接层是传统的多层感知器，在输出层使用的是softmax激活函数。“全连接（Fully Connected）”这个词表明前面层的所有神经元都与下一层的所有神经元连接。卷积和池化层的输出表示了输入图像的高级特征。全连接层的目的是为了使用这些特征把输入图像基于训练数据集进行分类。比如，在下面图中我们进行的图像分类有四个可能的输出结果（注意下图并没有显示全连接层的节点连接）。
 
除了分类，添加一个全连接层也（一般）是学习这些特征的非线性组合的简单方法。从卷积和池化层得到的大多数特征可能对分类任务有效，但这些特征的组合可能会更好。从全连接层得到的输出概率和为1。这个可以在输出层使用softmax作为激活函数进行保证。softmax函数输入一个任意大于0值的矢量，并把它们转换为零一之间的数值矢量，其和为一。

3．简单实例与总结
3.1 实例一与CNNs步骤总结
正如第2章讨论的，卷积+池化层的作用是从输入图像中提取特征，而全连接层的作用是分类器。注意在下面的图中，因为输入的图像是船，对于船这一类的目标概率是 1，而其他三类的目标概率是 0，即：
	输入图像 = 船
	目标矢量 = [0, 0, 1, 0]
 
完整的卷积网络的训练过程可以总结如下：
第一步：我们初始化所有的滤波器，使用随机值设置参数/权重。
第二步：网络接收一张训练图像作为输入，通过前向传播过程（卷积、ReLU 和池化操作，以及全连接层的前向传播），找到各个类的输出概率我们假设船这张图像的输出概率是 [0.2, 0.4, 0.1, 0.3]。因为对于第一张训练样本的权重是随机分配的，输出的概率也是随机的。
第三步：在输出层计算总误差（计算4类的和）。
Total Error = ∑½ (target probability – output probability) ²
第四步：使用反向传播算法，根据网络的权重计算误差的梯度，并使用梯度下降算法更新所有滤波器的值/权重以及参数的值，使输出误差最小化。权重的更新与它们对总误差的占比有关。当同样的图像再次作为输入，这时的输出概率可能会是 [0.1, 0.1, 0.7, 0.1]，这就与目标矢量 [0, 0, 1, 0] 更接近了。这表明网络已经通过调节权重/滤波器，可以正确对这张特定图像的分类，这样输出的误差就减小了。像滤波器数量、滤波器大小、网络结构等这样的参数，在第一步前都是固定的，在训练过程中保持不变——仅仅是滤波器矩阵的值和连接权重在更新。
第五步：对训练数据中所有的图像重复步骤 1 ~ 4。
上面的这些步骤可以训练CNNs—— 这意味着对于训练数据集中的图像，CNNs在更新了所有权重和参数后，已经优化为可以对这些图像进行正确分类。当一张新的（未见过的）图像作为CNNs的输入，网络将会再次进行前向传播过程，并输出各个类别的概率（对于新的图像，输出概率是使用已经在前面训练样本上优化分类的参数进行计算的）。如果我们的训练数据集非常的大，网络将会（有希望）对新的图像有很好的泛化，并把它们分到正确的类别中去。
注：在上面的例子中我们使用了两组卷积和池化层。然而，这些操作可以在一个CNNs中重复多次。实际上，很多表现很好的 CNNs拥有多达十几层的卷积和池化层！同时，每次卷积层后面不一定要有池化层，如下图：
 

3.2 实例二与可视化理解
一般而言，越多的卷积步骤，网络可以学到的识别特征就越复杂。比如，CNNs 的图像分类可能在第一层从原始像素中检测出边缘，然后在第二层使用边缘检测简单的形状，接着使用这些形状检测更高级的特征，比如更高层的人脸。
               
	实例二中通过识别手写数字8，展示CNNs的可视化过程。
 
输入图像包含 1024 个像素（32 x 32 大小），第一个卷积层（卷积层 1）由六个独特的 5x5 （步长为 1）的滤波器组成。如图可见，使用六个不同的滤波器得到一个深度为六的特征图。
卷积层1后面是池化层1，在卷积层1得到的六个特征图上分别进行2x2的最大池化（步长为2）的操作。
池化层1后面的是十六个5x5（步长为 1）的卷积滤波器，进行卷积操作。后面就是池化层2，进行2x2的最大池化（步长为2）的操作。这两层的概念和前面描述的一样。
接下来我们就到了三个全连接层。它们是：
	第一个全连接层有120个神经元。
	第二个全连接层有100个神经元。
	第三个全连接层有10个神经元，对应10个数字也就是输出层。
注意在下图中，输出层中的 10 个节点的各个都与第二个全连接层的所有 100 个节点相连（因此叫做全连接）。同时，注意在输出层那个唯一的亮的节点是如何对应于数字 “8” 的——这表明网络把我们的手写数字正确分类（越亮的节点表明从它得到的输出值越高，即，8 是所有数字中概率最高的）。
 
