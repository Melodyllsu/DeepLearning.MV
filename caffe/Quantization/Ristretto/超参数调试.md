参考链接：
https://blog.csdn.net/qyczyr/article/details/72835595  
https://blog.csdn.net/jiongnima/article/details/68929303   
https://baijiahao.baidu.com/s?id=1572697980489279&wfr=spider&for=pc  



## Solver的流程：

1.设计好需要优化的对象，以及用于学习的训练网络和用于评估的测试网络。（通过调用另外一个配置文件prototxt来进行）

2.通过forward和backward迭代的进行优化来跟新参数。

3.定期的评价测试网络。 （可设定多少次训练后，进行一次测试）

4.在优化过程中显示模型和solver的状态

## 在每一次的迭代过程中，solver做了这几步工作：

1、调用forward算法来计算最终的输出值，以及对应的loss

2、调用backward算法来计算每层的梯度

3、根据选用的slover方法，利用梯度进行参数更新

4、记录并保存每次迭代的学习率、快照，以及对应的状态。
## solver_funetune.prototxt
```
# Ristretto SqueezeNet Example
# Fine-tuning of 8-bit dynamic fixed point network

# 测试样本，如果有1000个测试样本，batch_size为25，那么需要40次才能完整的测试一次。 所以test_iter为40；这个数要大于等于40.
test_iter: 2000
# 间隔测试。每训练100次进行一次测试。
test_interval: 100
# 基础学习率
base_lr: 0.000001
# 每训练10次，在屏幕上显示一次。如果设置为0，则不显示。
display: 10
# 最大迭代次数。这个数设置太小，会导致没有收敛，精确度很低。设置太大，会导致震荡，浪费时间。
max_iter: 2000
# 这个参数乘上train.prototxt中的batch size是你实际使用的batch size。 相当于读取batchsize * itersize个图像才做一下gradient decent。
iter_size:16  #global batch size = batch_size * iter_size
# 学习率调整策略，fixed 保持base_lr不变
lr_policy: "fixed"
# 上一次梯度更新的权重
momentum: 0.9
momentum2: 0.999
delta: 0.00000001
# 权重衰减项，防止过拟合，weight_decay越大，抑制越大，w2,w3等系数越小，weight_decay越小，抑制越小，w2,w3等系数越大 
weight_decay: 0.0002
# 快照。将训练出来的model和solver状态进行保存，snapshot用于设置训练多少次后进行保存，默认为0，不保存。snapshot_prefix设置保存路径。
snapshot: 100
snapshot_prefix: "models/VggNet/RistrettoDemo/vggnet"
# 设置运行模式。默认为GPU,如果你没有GPU,则需要改成CPU,否则会出错。
solver_mode: GPU
random_seed: 42
#  设置深度网络模型，文件的路径要从caffe的根目录开始，其它的所有配置都是这样
net: "models/VggNet/RistrettoDemo/quantized.prototxt"
# 进行40次测试迭代显示平均精度
average_loss: 40
# 表示是否可以用上次保存的snapshot来继续训练，如果为True,则下次开始训练的时候，caffe会自动从这个目录下加载最近一次迭代的模型，继续训练，以节省时间
test_initialization: true
# 优化算法选择
solver_type: ADAM
```
- 学习策略  
fixed:　　 保持base_lr不变.  
step: 　　 如果设置为step,则还需要设置一个stepsize,  返回 base_lr * gamma ^ (floor(iter / stepsize)),其中iter表示当前的迭代次数  
exp:   　　返回base_lr * gamma ^ iter， iter为当前迭代次数  
inv:　　    如果设置为inv,还需要设置一个power, 返回base_lr * (1 + gamma * iter) ^ (- power)  
multistep: 如果设置为multistep,则还需要设置一个stepvalue。这个参数和step很相似，step是均匀等间隔变化，而multistep则是根据                                   stepvalue值变化  
poly: 　　  学习率进行多项式误差, 返回 base_lr (1 - iter/max_iter) ^ (power)  
sigmoid:　学习率进行sigmod衰减，返回 base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))     
 
参考地址：  
https://blog.csdn.net/cuijyer/article/details/78195178  

1. step,则学习率的变化规则为 base_lr * gamma ^ (floor(iter / stepsize))
```
base_lr: 0.01 
lr_policy: "step"
gamma: 0.1   
stepsize: 1000  
max_iter: 3500 
momentum: 0.9
```
即前1000次迭代，学习率为0.01; 第1001-2000次迭代，学习率为0.001; 第2001-3000次迭代，学习率为0.00001，第3001-3500次迭代，学习率为10-5  
2. inv  
```
base_lr: 0.01
lr_policy: "inv"
gamma: 0.0001
power: 0.75
``` 
3. multistep   
```
base_lr: 0.01
lr_policy: "multistep"
gamma: 0.5
stepvalue: 1000
stepvalue: 3000
stepvalue: 4000
stepvalue: 4500
stepvalue: 5000
max_iter: 6000
```
4. exp  
```
base_lr: 0.01
lr_policy: "exp"
gamma: 0.1
max_iter: 100
```
5. ploy  
```
base_lr: 0.01
lr_policy: "poly"
power: 0.5
max_iter: 10000
```
6. sigmoid  
```
base_lr: 0.01
lr_policy: "sigmoid"
gamma: -0.001
stepsize: 5000
max_iter: 10000
```


- 优化算法选择  
到目前的版本，caffe提供了六种优化算法来求解最优参数，在solver配置文件中，通过设置type类型来选择。  
Stochastic Gradient Descent (type: "SGD"),  
AdaDelta (type: "AdaDelta"),  
Adaptive Gradient (type: "AdaGrad"),  
Adam (type: "Adam"),   
Nesterov’s Accelerated Gradient (type: "Nesterov"),  
RMSprop (type: "RMSProp")    
参考地址：https://blog.csdn.net/sinat_22336563/article/details/70305908  
https://blog.csdn.net/wfei101/article/details/79964583
1. SGD 随机梯度下降
   随机梯度下降（Stochastic gradient descent）是在梯度下降法（gradient descent）的基础上发展起来的，梯度下降法也叫最速下降法    
   在深度学习中使用SGD，比较好的初始化参数的策略是把学习率设为0.01左右（base_lr: 0.01)，在训练的过程中，如果loss开始出现稳定水平时，对学习率乘以一个常数因子（gamma），这样的过程重复多次。  
   对于momentum，一般取值在0.5--0.99之间。通常设为0.9，momentum可以让使用SGD的深度学习方法更加稳定以及快速。
2. AdaDelta是一种”鲁棒的学习率方法“，是基于梯度的优化方法（like SGD）。  
```
net: "examples/mnist/lenet_train_test.prototxt"
test_iter: 100
test_interval: 500
base_lr: 1.0
lr_policy: "fixed"
momentum: 0.95
weight_decay: 0.0005
display: 100
max_iter: 10000
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet_adadelta"
solver_mode: GPU
type: "AdaDelta"
delta: 1e-6
```
设置solver type为Adadelta时，需要设置delta的值。  
3. AdaGrad 自适应梯度（adaptive gradient）是基于梯度的优化方法（like SGD）  
```
net: "examples/mnist/mnist_autoencoder.prototxt"
test_state: { stage: 'test-on-train' }
test_iter: 500
test_state: { stage: 'test-on-test' }
test_iter: 100
test_interval: 500
test_compute_loss: true
base_lr: 0.01
lr_policy: "fixed"
display: 100
max_iter: 65000
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "examples/mnist/mnist_autoencoder_adagrad_train"
# solver mode: CPU or GPU
solver_mode: GPU
type: "AdaGrad"
```

4. Adam   
使用动量进行学习率衰减
Adam集RMSprop、Adadelta之大成，是目前使用最多的优化算法。原始的SGD收敛耗时太长，而且对局部最小值没有丝毫抵抗能力，所以自适应学习率的算法更优。

5. NAG Nesterov 的加速梯度法（Nesterov’s accelerated gradient）作为凸优化中最理想的方法，其收敛速度非常快。  
```
net: "examples/mnist/mnist_autoencoder.prototxt"
test_state: { stage: 'test-on-train' }
test_iter: 500
test_state: { stage: 'test-on-test' }
test_iter: 100
test_interval: 500
test_compute_loss: true
base_lr: 0.01
lr_policy: "step"
gamma: 0.1
stepsize: 10000
display: 100
max_iter: 65000
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "examples/mnist/mnist_autoencoder_nesterov_train"
momentum: 0.95
# solver mode: CPU or GPU
solver_mode: GPU
type: "Nesterov"
```
6. RMSprop  也是一种基于梯度的优化方法（like SGD）  
```
net: "examples/mnist/lenet_train_test.prototxt"
test_iter: 100
test_interval: 500
base_lr: 1.0
lr_policy: "fixed"
momentum: 0.95
weight_decay: 0.0005
display: 100
max_iter: 10000
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet_adadelta"
solver_mode: GPU
type: "RMSProp"
rms_decay: 0.98
```

# 测试数据
base_lr
0.000001
0.00001//0.0001//0.0000001
max_iter
2000
4000//1000
iter_size
32
16//64
lr_policy
fixed

weight_decay
0.0002

solver_type
ADAM


